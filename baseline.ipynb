{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tIfeZ2Cbual",
        "outputId": "6ab6a82c-2c51-4166-c3a4-6e2db14163b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:Dataset loaded and cleaned. Total instances: 3226\n",
            "INFO:root:BoW vocabulary size: 5000 tokens\n",
            "INFO:root:Sample BoW features (first 20 tokens): ['a' 'ability' 'able' 'about' 'above' 'abroad' 'absolute' 'absolutely'\n",
            " 'abt' 'abuse' 'academic' 'academy' 'accept' 'acceptance' 'accepted'\n",
            " 'accepting' 'access' 'accessories' 'accident' 'accommodate']\n",
            "INFO:root:Sample BoW vector (first train instance): [[8 0 0 ... 0 0 0]]\n",
            "INFO:root:TF-IDF vocabulary size: 5000 tokens\n",
            "INFO:root:Sample TF-IDF features (first 20 tokens): ['a' 'ability' 'able' 'about' 'above' 'abroad' 'absolute' 'absolutely'\n",
            " 'abt' 'abuse' 'academic' 'academy' 'accept' 'acceptance' 'accepted'\n",
            " 'accepting' 'access' 'accessories' 'accident' 'accommodate']\n",
            "INFO:root:Sample TF-IDF vector (first train instance): [[0.15224339 0.         0.         ... 0.         0.         0.        ]]\n",
            "INFO:root:Training Logistic Regression (TF-IDF)...\n",
            "INFO:root:Logistic Regression (TF-IDF) Accuracy: 0.7121\n",
            "INFO:root:Logistic Regression (TF-IDF) Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           M       0.73      0.71      0.72       336\n",
            "           F       0.69      0.71      0.70       310\n",
            "\n",
            "    accuracy                           0.71       646\n",
            "   macro avg       0.71      0.71      0.71       646\n",
            "weighted avg       0.71      0.71      0.71       646\n",
            "\n",
            "INFO:root:Training Random Forest (TF-IDF)...\n",
            "INFO:root:Random Forest (TF-IDF) Accuracy: 0.6703\n",
            "INFO:root:Random Forest (TF-IDF) Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           M       0.69      0.68      0.68       336\n",
            "           F       0.65      0.66      0.66       310\n",
            "\n",
            "    accuracy                           0.67       646\n",
            "   macro avg       0.67      0.67      0.67       646\n",
            "weighted avg       0.67      0.67      0.67       646\n",
            "\n",
            "INFO:root:Training Linear SVM (TF-IDF)...\n",
            "INFO:root:Linear SVM (TF-IDF) Accuracy: 0.7043\n",
            "INFO:root:Linear SVM (TF-IDF) Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           M       0.73      0.69      0.71       336\n",
            "           F       0.68      0.72      0.70       310\n",
            "\n",
            "    accuracy                           0.70       646\n",
            "   macro avg       0.70      0.70      0.70       646\n",
            "weighted avg       0.71      0.70      0.70       646\n",
            "\n",
            "INFO:root:Training Gradient Boosting (TF-IDF)...\n",
            "INFO:root:Gradient Boosting (TF-IDF) Accuracy: 0.6811\n",
            "INFO:root:Gradient Boosting (TF-IDF) Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           M       0.68      0.72      0.70       336\n",
            "           F       0.68      0.64      0.66       310\n",
            "\n",
            "    accuracy                           0.68       646\n",
            "   macro avg       0.68      0.68      0.68       646\n",
            "weighted avg       0.68      0.68      0.68       646\n",
            "\n",
            "INFO:root:Training Logistic Regression (BoW)...\n",
            "INFO:root:Logistic Regression (BoW) Accuracy: 0.6718\n",
            "INFO:root:Logistic Regression (BoW) Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           M       0.70      0.65      0.67       336\n",
            "           F       0.65      0.70      0.67       310\n",
            "\n",
            "    accuracy                           0.67       646\n",
            "   macro avg       0.67      0.67      0.67       646\n",
            "weighted avg       0.67      0.67      0.67       646\n",
            "\n",
            "INFO:root:Training Random Forest (BoW)...\n",
            "INFO:root:Random Forest (BoW) Accuracy: 0.6873\n",
            "INFO:root:Random Forest (BoW) Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           M       0.68      0.74      0.71       336\n",
            "           F       0.69      0.63      0.66       310\n",
            "\n",
            "    accuracy                           0.69       646\n",
            "   macro avg       0.69      0.68      0.68       646\n",
            "weighted avg       0.69      0.69      0.69       646\n",
            "\n",
            "INFO:root:Training Multinomial Naive Bayes (BoW)...\n",
            "INFO:root:Multinomial Naive Bayes (BoW) Accuracy: 0.6966\n",
            "INFO:root:Multinomial Naive Bayes (BoW) Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           M       0.77      0.59      0.67       336\n",
            "           F       0.65      0.81      0.72       310\n",
            "\n",
            "    accuracy                           0.70       646\n",
            "   macro avg       0.71      0.70      0.69       646\n",
            "weighted avg       0.71      0.70      0.69       646\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import sys\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from typing import Tuple, Any, Dict\n",
        "\n",
        "# Set up logging\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# handler\n",
        "if not logger.handlers:\n",
        "    ch = logging.StreamHandler(sys.stdout)\n",
        "    ch.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    ch.setFormatter(formatter)\n",
        "    logger.addHandler(ch)\n",
        "\n",
        "def load_and_clean_dataset(file_path: str) -> pd.DataFrame:\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, header=0, names=['text', 'gender'])\n",
        "        df = df.dropna(subset=['text', 'gender'])\n",
        "        df['gender'] = df['gender'].astype(str).str.strip().str.upper()\n",
        "        df = df[df['gender'].isin(['M', 'F'])]\n",
        "        df['label'] = df['gender'].map({'M': 0, 'F': 1})\n",
        "        logging.info(f\"Dataset loaded and cleaned. Total instances: {df.shape[0]}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error loading or cleaning the dataset\", exc_info=True)\n",
        "        raise e\n",
        "\n",
        "def split_dataset(df: pd.DataFrame, test_size: float = 0.2, random_state: int = 42) -> Tuple:\n",
        "    return train_test_split(df, test_size=test_size, stratify=df['label'], random_state=random_state)\n",
        "\n",
        "def build_vectorizers(max_features: int = 5000) -> Dict[str, Any]:\n",
        "    token_pattern = r\"(?u)\\b[a-zA-Z]+\\b\"\n",
        "    bow_vectorizer = CountVectorizer(max_features=max_features, token_pattern=token_pattern)\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=max_features, token_pattern=token_pattern)\n",
        "    return {\"bow\": bow_vectorizer, \"tfidf\": tfidf_vectorizer}\n",
        "\n",
        "def vectorize_data(vectorizer: Any, texts_train: pd.Series, texts_test: pd.Series) -> Tuple:\n",
        "    X_train = vectorizer.fit_transform(texts_train)\n",
        "    X_test = vectorizer.transform(texts_test)\n",
        "    return X_train, X_test\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, model_name: str) -> None:\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred, target_names=['M', 'F'])\n",
        "    logging.info(f\"{model_name} Accuracy: {acc:.4f}\")\n",
        "    logging.info(f\"{model_name} Classification Report:\\n{report}\")\n",
        "    sys.stdout.flush()  # Ensure output is flushed\n",
        "\n",
        "def main():\n",
        "    file_path = '/data/raw/gender-classification.csv'\n",
        "    df = load_and_clean_dataset(file_path)\n",
        "\n",
        "    df_train, df_test = split_dataset(df)\n",
        "    y_train = df_train['label']\n",
        "    y_test = df_test['label']\n",
        "\n",
        "    vectorizers = build_vectorizers(max_features=5000)\n",
        "\n",
        "    # BoW Vectorization\n",
        "    X_train_bow, X_test_bow = vectorize_data(vectorizers[\"bow\"], df_train['text'], df_test['text'])\n",
        "    vocab_bow = vectorizers[\"bow\"].get_feature_names_out()\n",
        "    logging.info(f\"BoW vocabulary size: {len(vocab_bow)} tokens\")\n",
        "    logging.info(f\"Sample BoW features (first 20 tokens): {vocab_bow[:20]}\")\n",
        "    logging.info(f\"Sample BoW vector (first train instance): {X_train_bow[0].toarray()}\")\n",
        "\n",
        "    # TF-IDF Vectorization\n",
        "    X_train_tfidf, X_test_tfidf = vectorize_data(vectorizers[\"tfidf\"], df_train['text'], df_test['text'])\n",
        "    vocab_tfidf = vectorizers[\"tfidf\"].get_feature_names_out()\n",
        "    logging.info(f\"TF-IDF vocabulary size: {len(vocab_tfidf)} tokens\")\n",
        "    logging.info(f\"Sample TF-IDF features (first 20 tokens): {vocab_tfidf[:20]}\")\n",
        "    logging.info(f\"Sample TF-IDF vector (first train instance): {X_train_tfidf[0].toarray()}\")\n",
        "\n",
        "    # Models on TF-IDF features\n",
        "    models_tfidf = {\n",
        "        \"Logistic Regression (TF-IDF)\": LogisticRegression(max_iter=1000),\n",
        "        \"Random Forest (TF-IDF)\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        \"Linear SVM (TF-IDF)\": LinearSVC(max_iter=10000),\n",
        "        \"Gradient Boosting (TF-IDF)\": GradientBoostingClassifier(random_state=42)\n",
        "    }\n",
        "    for name, model in models_tfidf.items():\n",
        "        logging.info(f\"Training {name}...\")\n",
        "        model.fit(X_train_tfidf, y_train)\n",
        "        evaluate_model(model, X_test_tfidf, y_test, name)\n",
        "\n",
        "    # Models on BoW features\n",
        "    models_bow = {\n",
        "        \"Logistic Regression (BoW)\": LogisticRegression(max_iter=1000),\n",
        "        \"Random Forest (BoW)\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        \"Multinomial Naive Bayes (BoW)\": MultinomialNB()\n",
        "    }\n",
        "    for name, model in models_bow.items():\n",
        "        logging.info(f\"Training {name}...\")\n",
        "        model.fit(X_train_bow, y_train)\n",
        "        evaluate_model(model, X_test_bow, y_test, name)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
